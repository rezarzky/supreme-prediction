{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cshu3EV8KvOR"},"outputs":[],"source":["import pandas as pd\n","from ast import literal_eval\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67JifBISKvOX","outputId":"81e99328-baa5-42e9-b52d-eb6911fc25a2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\rezar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\rezar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\rezar\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","import re\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from nltk.stem import PorterStemmer"]},{"cell_type":"markdown","metadata":{"id":"6hVWszEvKvOZ"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBNIoPDmKvOb"},"outputs":[],"source":["path = 'dataset/' #change to your data location\n","# load downloaded data\n","df_convos = pd.read_csv(path+'/conversations.csv')\n","df_speakers = pd.read_csv(path+'/speakers.csv')\n","df_utts = pd.read_csv(path+'/utterances.csv')\n","df_cases = pd.read_json(path_or_buf=path+'/cases.jsonl', lines=True)\n","df_cases = df_cases[(df_cases['year'] >= 2011) & (df_cases['year'] <= 2018) & (df_cases['win_side'].isin([0,1]))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8X2XQByeKvOb","outputId":"04aad86a-e5b3-463a-da4e-f4dd820fae93"},"outputs":[{"data":{"text/plain":["1.0    400\n","0.0    201\n","Name: win_side, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# count number win/lose cases\n","df_cases['win_side'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82cf8kTYKvOc"},"outputs":[],"source":["# combine text from all utterances in a conversation back into one string based on the conversation_id, coount how many utterances per conversation\n","utt_per_conv = df_utts.groupby('conversation_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n","utt_per_conv['num_utterances'] = df_utts.groupby('conversation_id')['text'].count().reset_index()['text']\n","\n","# add the combined text to the conversations dataframe, merge on conversation_id in utt_per_conv and id in df_convo\n","df_convos_utt = df_convos.merge(utt_per_conv, left_on='id', right_on='conversation_id', how='left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uYpaIivKvOd","outputId":"ddfc90fe-ad54-4b70-892c-06726599529a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year</th>\n","      <th>citation</th>\n","      <th>title</th>\n","      <th>petitioner</th>\n","      <th>respondent</th>\n","      <th>docket_no</th>\n","      <th>court</th>\n","      <th>decided_date</th>\n","      <th>url</th>\n","      <th>...</th>\n","      <th>win_side_detail</th>\n","      <th>scdb_docket_id</th>\n","      <th>votes</th>\n","      <th>votes_detail</th>\n","      <th>is_eq_divided</th>\n","      <th>votes_side</th>\n","      <th>meta.case_id</th>\n","      <th>text</th>\n","      <th>num_conversations</th>\n","      <th>num_utterances</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2011_11-1179</td>\n","      <td>2011</td>\n","      <td>567 US _</td>\n","      <td>American Tradition Partnership, Inc. v. Bullock</td>\n","      <td>American Tradition Partnership, Inc.</td>\n","      <td>Steve Bullock, Attorney General of Montana, et...</td>\n","      <td>11-1179</td>\n","      <td>Roberts Court</td>\n","      <td>Jun 25, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-1179</td>\n","      <td>...</td>\n","      <td>3.0</td>\n","      <td>2011-073-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2011_11-182</td>\n","      <td>2011</td>\n","      <td>567 US _</td>\n","      <td>Arizona v. United States</td>\n","      <td>Arizona et al.</td>\n","      <td>United States</td>\n","      <td>11-182</td>\n","      <td>Roberts Court</td>\n","      <td>Jun 25, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-182</td>\n","      <td>...</td>\n","      <td>7.0</td>\n","      <td>2011-075-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...</td>\n","      <td>2011_11-182</td>\n","      <td>We'll hear argument this morning in Case 11-18...</td>\n","      <td>1.0</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2011_11-161</td>\n","      <td>2011</td>\n","      <td>566 US _</td>\n","      <td>Armour v. City of Indianapolis</td>\n","      <td>Christine Armour</td>\n","      <td>City of Indianapolis</td>\n","      <td>11-161</td>\n","      <td>Roberts Court</td>\n","      <td>Jun 4, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-161</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>2011-062-01</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>2011_11-161</td>\n","      <td>We will hear argument this morning in case 11-...</td>\n","      <td>1.0</td>\n","      <td>239.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows Ã— 25 columns</p>\n","</div>"],"text/plain":["             id  year  citation  \\\n","0  2011_11-1179  2011  567 US _   \n","1   2011_11-182  2011  567 US _   \n","2   2011_11-161  2011  566 US _   \n","\n","                                             title  \\\n","0  American Tradition Partnership, Inc. v. Bullock   \n","1                         Arizona v. United States   \n","2                   Armour v. City of Indianapolis   \n","\n","                             petitioner  \\\n","0  American Tradition Partnership, Inc.   \n","1                        Arizona et al.   \n","2                      Christine Armour   \n","\n","                                          respondent docket_no          court  \\\n","0  Steve Bullock, Attorney General of Montana, et...   11-1179  Roberts Court   \n","1                                      United States    11-182  Roberts Court   \n","2                               City of Indianapolis    11-161  Roberts Court   \n","\n","   decided_date                                      url  ... win_side_detail  \\\n","0  Jun 25, 2012  https://www.oyez.org/cases/2011/11-1179  ...             3.0   \n","1  Jun 25, 2012   https://www.oyez.org/cases/2011/11-182  ...             7.0   \n","2   Jun 4, 2012   https://www.oyez.org/cases/2011/11-161  ...             2.0   \n","\n","   scdb_docket_id                                              votes  \\\n","0     2011-073-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","1     2011-075-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","2     2011-062-01  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...   \n","\n","                                        votes_detail  is_eq_divided  \\\n","0  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","1  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","2  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...            0.0   \n","\n","                                          votes_side meta.case_id  \\\n","0  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...          NaN   \n","1  {'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...  2011_11-182   \n","2  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...  2011_11-161   \n","\n","                                                text num_conversations  \\\n","0                                                NaN               NaN   \n","1  We'll hear argument this morning in Case 11-18...               1.0   \n","2  We will hear argument this morning in case 11-...               1.0   \n","\n","   num_utterances  \n","0             NaN  \n","1           295.0  \n","2           239.0  \n","\n","[3 rows x 25 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# combine text from all conversation in a cases into one string based on the meta.case_id\n","conv_per_case = df_convos_utt.groupby('meta.case_id')['text'].apply(lambda x: ' '.join(x)).reset_index()\n","conv_per_case['num_conversations'] = df_convos_utt.groupby('meta.case_id')['text'].count().reset_index()['text']\n","conv_per_case['num_utterances'] = df_convos_utt.groupby('meta.case_id')['num_utterances'].sum().reset_index()['num_utterances']\n","\n","# add the combined text case dataframe, merge on meta.case_id and id\n","df_cases_convo = df_cases.merge(conv_per_case, left_on='id', right_on='meta.case_id', how='left')\n","df_cases_convo.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCiM_ft-KvOd","outputId":"e7e23a13-56bb-4cf7-891c-2f28f05978f2"},"outputs":[{"data":{"text/plain":["521"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_cases_convo.dropna(subset=['text'], inplace=True)\n","df_cases_convo.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ay1JBN0GKvOe","outputId":"330fb7b0-da26-40ac-9151-6582c7cd8412"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>year</th>\n","      <th>citation</th>\n","      <th>title</th>\n","      <th>petitioner</th>\n","      <th>respondent</th>\n","      <th>docket_no</th>\n","      <th>court</th>\n","      <th>decided_date</th>\n","      <th>url</th>\n","      <th>...</th>\n","      <th>win_side_detail</th>\n","      <th>scdb_docket_id</th>\n","      <th>votes</th>\n","      <th>votes_detail</th>\n","      <th>is_eq_divided</th>\n","      <th>votes_side</th>\n","      <th>meta.case_id</th>\n","      <th>text</th>\n","      <th>num_conversations</th>\n","      <th>num_utterances</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>2011_11-182</td>\n","      <td>2011</td>\n","      <td>567 US _</td>\n","      <td>Arizona v. United States</td>\n","      <td>Arizona et al.</td>\n","      <td>United States</td>\n","      <td>11-182</td>\n","      <td>Roberts Court</td>\n","      <td>Jun 25, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-182</td>\n","      <td>...</td>\n","      <td>7.0</td>\n","      <td>2011-075-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...</td>\n","      <td>2011_11-182</td>\n","      <td>We'll hear argument this morning in Case 11-18...</td>\n","      <td>1.0</td>\n","      <td>295.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2011_11-161</td>\n","      <td>2011</td>\n","      <td>566 US _</td>\n","      <td>Armour v. City of Indianapolis</td>\n","      <td>Christine Armour</td>\n","      <td>City of Indianapolis</td>\n","      <td>11-161</td>\n","      <td>Roberts Court</td>\n","      <td>Jun 4, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-161</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>2011-062-01</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>2011_11-161</td>\n","      <td>We will hear argument this morning in case 11-...</td>\n","      <td>1.0</td>\n","      <td>239.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2011_11-159</td>\n","      <td>2011</td>\n","      <td>566 US _</td>\n","      <td>Astrue v. Capato</td>\n","      <td>Michael J. Astrue, Commissioner of Social Secu...</td>\n","      <td>Karen K. Capato</td>\n","      <td>11-159</td>\n","      <td>Roberts Court</td>\n","      <td>May 21, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/11-159</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>2011-054-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>2011_11-159</td>\n","      <td>We will hear argument first this morning in Ca...</td>\n","      <td>1.0</td>\n","      <td>201.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2011_10-1320</td>\n","      <td>2011</td>\n","      <td>566 US _</td>\n","      <td>Blueford v. Arkansas</td>\n","      <td>Alex Blueford</td>\n","      <td>Arkansas</td>\n","      <td>10-1320</td>\n","      <td>Roberts Court</td>\n","      <td>May 24, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/10-1320</td>\n","      <td>...</td>\n","      <td>2.0</td>\n","      <td>2011-057-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...</td>\n","      <td>2011_10-1320</td>\n","      <td>We'll hear argument next in Case 10-1320, Blue...</td>\n","      <td>1.0</td>\n","      <td>191.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2011_10-844</td>\n","      <td>2011</td>\n","      <td>566 US _</td>\n","      <td>Caraco Pharmaceutical Laboratories, Ltd. v. No...</td>\n","      <td>Caraco Pharmaceutical Laboratories, Ltd., et al.</td>\n","      <td>Novo Nordisk A/S, et al.</td>\n","      <td>10-844</td>\n","      <td>Roberts Court</td>\n","      <td>Apr 17, 2012</td>\n","      <td>https://www.oyez.org/cases/2011/10-844</td>\n","      <td>...</td>\n","      <td>4.0</td>\n","      <td>2011-047-01</td>\n","      <td>{'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>0.0</td>\n","      <td>{'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...</td>\n","      <td>2011_10-844</td>\n","      <td>We'll hear argument first this morning in Case...</td>\n","      <td>1.0</td>\n","      <td>210.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 25 columns</p>\n","</div>"],"text/plain":["             id  year  citation  \\\n","1   2011_11-182  2011  567 US _   \n","2   2011_11-161  2011  566 US _   \n","3   2011_11-159  2011  566 US _   \n","4  2011_10-1320  2011  566 US _   \n","6   2011_10-844  2011  566 US _   \n","\n","                                               title  \\\n","1                           Arizona v. United States   \n","2                     Armour v. City of Indianapolis   \n","3                                   Astrue v. Capato   \n","4                               Blueford v. Arkansas   \n","6  Caraco Pharmaceutical Laboratories, Ltd. v. No...   \n","\n","                                          petitioner  \\\n","1                                     Arizona et al.   \n","2                                   Christine Armour   \n","3  Michael J. Astrue, Commissioner of Social Secu...   \n","4                                      Alex Blueford   \n","6   Caraco Pharmaceutical Laboratories, Ltd., et al.   \n","\n","                 respondent docket_no          court  decided_date  \\\n","1             United States    11-182  Roberts Court  Jun 25, 2012   \n","2      City of Indianapolis    11-161  Roberts Court   Jun 4, 2012   \n","3           Karen K. Capato    11-159  Roberts Court  May 21, 2012   \n","4                  Arkansas   10-1320  Roberts Court  May 24, 2012   \n","6  Novo Nordisk A/S, et al.    10-844  Roberts Court  Apr 17, 2012   \n","\n","                                       url  ... win_side_detail  \\\n","1   https://www.oyez.org/cases/2011/11-182  ...             7.0   \n","2   https://www.oyez.org/cases/2011/11-161  ...             2.0   \n","3   https://www.oyez.org/cases/2011/11-159  ...             4.0   \n","4  https://www.oyez.org/cases/2011/10-1320  ...             2.0   \n","6   https://www.oyez.org/cases/2011/10-844  ...             4.0   \n","\n","   scdb_docket_id                                              votes  \\\n","1     2011-075-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","2     2011-062-01  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...   \n","3     2011-054-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","4     2011-057-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","6     2011-047-01  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...   \n","\n","                                        votes_detail  is_eq_divided  \\\n","1  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","2  {'j__john_g_roberts_jr': 2.0, 'j__antonin_scal...            0.0   \n","3  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","4  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","6  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...            0.0   \n","\n","                                          votes_side  meta.case_id  \\\n","1  {'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...   2011_11-182   \n","2  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...   2011_11-161   \n","3  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...   2011_11-159   \n","4  {'j__john_g_roberts_jr': 0.0, 'j__antonin_scal...  2011_10-1320   \n","6  {'j__john_g_roberts_jr': 1.0, 'j__antonin_scal...   2011_10-844   \n","\n","                                                text num_conversations  \\\n","1  We'll hear argument this morning in Case 11-18...               1.0   \n","2  We will hear argument this morning in case 11-...               1.0   \n","3  We will hear argument first this morning in Ca...               1.0   \n","4  We'll hear argument next in Case 10-1320, Blue...               1.0   \n","6  We'll hear argument first this morning in Case...               1.0   \n","\n","   num_utterances  \n","1           295.0  \n","2           239.0  \n","3           201.0  \n","4           191.0  \n","6           210.0  \n","\n","[5 rows x 25 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df_cases_convo.head()"]},{"cell_type":"markdown","metadata":{"id":"BU3pXcoTKvOf"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9awcypdKvOg"},"outputs":[],"source":["# Cleaning the text\n","def preprocess_text(text):\n","    text = text.lower() # Lowercase the text\n","    text = re.sub('[^a-z]+', ' ', text)  # Remove special characters and numbers\n","    text = re.sub(r'\\b\\w{1,3}\\b', '', text) # Remove words with length less than 3\n","    words = nltk.word_tokenize(text) # Tokenize the text\n","    stop_words = set(stopwords.words('english')) # Remove stopwords\n","    words = [word for word in words if word not in stop_words]\n","    #lemmatizer = WordNetLemmatizer() # Lemmatize the words comment because slow\n","    #words = [lemmatizer.lemmatize(word) for word in words]\n","    stemmer = PorterStemmer() # Stem the words\n","    words = [stemmer.stem(word) for word in words]\n","    text = ' '.join(words) # Reconstruct the text\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SV4ahInIKvOg","outputId":"dda69299-7836-466b-e25d-95090831afbb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>win_side</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>We'll hear argument this morning in Case 11-18...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>We will hear argument this morning in case 11-...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>We will hear argument first this morning in Ca...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  win_side\n","1  We'll hear argument this morning in Case 11-18...       0.0\n","2  We will hear argument this morning in case 11-...       0.0\n","3  We will hear argument first this morning in Ca...       1.0"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["text = df_cases_convo.loc[:,['text','win_side']]\n","text.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RleO-mFbKvOh"},"outputs":[],"source":["text.to_csv('text.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EypB4kWfKvOh","outputId":"0fef7932-f3a9-48a6-a2ba-42f5ee83081b"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>win_side</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>hear argument morn case arizona unit state cle...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  win_side\n","1  hear argument morn case arizona unit state cle...       0.0"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["text['text'] = text['text'].apply(preprocess_text) #apply preprocess\n","text.head(1)\n","#text.to_csv('text_clean.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL_3Mjx8KvOh","outputId":"06c8f5ad-a3c0-4f55-cb58-9ee4108bbe01"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>win_side</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hear argument morn case arizona unit state cle...</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  win_side\n","0  hear argument morn case arizona unit state cle...       0.0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["text = pd.read_csv('text_clean.csv')\n","text.head(1)"]},{"cell_type":"markdown","metadata":{"id":"x87JQw66KvOi"},"source":["## Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWgpOtOUKvOi","outputId":"60016241-efe7-44ec-9f48-3cf74f049d18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.6655574043261231\n"]}],"source":["# Calculate The Baseline for Accuracy, Precision, Recall, F1\n","accuracy = df_cases['win_side'].value_counts()[1]/df_cases['win_side'].shape[0]\n","print('Accuracy: ', accuracy)"]},{"cell_type":"markdown","metadata":{"id":"B-CfeufdKvOi"},"source":["## Model Selection and Vectorize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MB4sAwC6KvOi"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n","\n","\n","def Classifier(X_train, X_test, y_train, y_test):\n","\n","    # Train and evaluate the classifiers\n","    classifiers = {\n","        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n","        \"Naive Bayes\": MultinomialNB(),\n","        \"Linear SVC\": LinearSVC(),\n","        \"Random Forest\": RandomForestClassifier(),\n","        \"Perceptron\": Perceptron(),\n","    }\n","\n","    results = []\n","\n","    for classifier_name, classifier in classifiers.items():\n","\n","        # Train the classifier\n","        classifier.fit(X_train, y_train)\n","\n","        # Make predictions on the test set\n","        y_pred = classifier.predict(X_test)\n","\n","        # Add the scores to the results dictionary\n","        results.append({\n","            'classifier': classifier_name,\n","            'accuracy': accuracy_score(y_test, y_pred),\n","            'f1': f1_score(y_test, y_pred),\n","            'precision': precision_score(y_test, y_pred),\n","            'recall': recall_score(y_test, y_pred)\n","        })\n","    return pd.DataFrame(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oa9mF8HlKvOj"},"outputs":[],"source":["def Vectorize(vectorizer, X, y):\n","    X = vectorizer.fit_transform(X)\n","    y = y\n","    return X, y    "]},{"cell_type":"markdown","metadata":{"id":"8lpluAXaKvOj"},"source":["## Run Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dV3Pfs62KvOj","outputId":"f9eb89a0-c2e4-455c-f901-cf6f454e1d78"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>classifier</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Logistic Regression</td>\n","      <td>0.704762</td>\n","      <td>0.822857</td>\n","      <td>0.727273</td>\n","      <td>0.947368</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive Bayes</td>\n","      <td>0.723810</td>\n","      <td>0.839779</td>\n","      <td>0.723810</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Linear SVC</td>\n","      <td>0.628571</td>\n","      <td>0.745098</td>\n","      <td>0.740260</td>\n","      <td>0.750000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Random Forest</td>\n","      <td>0.714286</td>\n","      <td>0.831461</td>\n","      <td>0.725490</td>\n","      <td>0.973684</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Perceptron</td>\n","      <td>0.628571</td>\n","      <td>0.731034</td>\n","      <td>0.768116</td>\n","      <td>0.697368</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            classifier  accuracy        f1  precision    recall\n","0  Logistic Regression  0.704762  0.822857   0.727273  0.947368\n","1          Naive Bayes  0.723810  0.839779   0.723810  1.000000\n","2           Linear SVC  0.628571  0.745098   0.740260  0.750000\n","3        Random Forest  0.714286  0.831461   0.725490  0.973684\n","4           Perceptron  0.628571  0.731034   0.768116  0.697368"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["# Vectorize the text using TF-IDF\n","vectorizer = TfidfVectorizer(min_df=5, max_df=0.7)\n","X, y = Vectorize(vectorizer, text['text'], text['win_side'])\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","Classifier(X_train, X_test, y_train, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kJL5M0c5KvOj","outputId":"96deb9ba-4a77-44eb-cc0e-8fe22a3b8d3a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>classifier</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Logistic Regression</td>\n","      <td>0.580952</td>\n","      <td>0.671642</td>\n","      <td>0.633803</td>\n","      <td>0.714286</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Naive Bayes</td>\n","      <td>0.571429</td>\n","      <td>0.634146</td>\n","      <td>0.650000</td>\n","      <td>0.619048</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Linear SVC</td>\n","      <td>0.590476</td>\n","      <td>0.661417</td>\n","      <td>0.656250</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Random Forest</td>\n","      <td>0.590476</td>\n","      <td>0.742515</td>\n","      <td>0.596154</td>\n","      <td>0.984127</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Perceptron</td>\n","      <td>0.571429</td>\n","      <td>0.651163</td>\n","      <td>0.636364</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            classifier  accuracy        f1  precision    recall\n","0  Logistic Regression  0.580952  0.671642   0.633803  0.714286\n","1          Naive Bayes  0.571429  0.634146   0.650000  0.619048\n","2           Linear SVC  0.590476  0.661417   0.656250  0.666667\n","3        Random Forest  0.590476  0.742515   0.596154  0.984127\n","4           Perceptron  0.571429  0.651163   0.636364  0.666667"]},"execution_count":161,"metadata":{},"output_type":"execute_result"}],"source":["# Vectorize the text using CountVectorizer\n","vectorizer = CountVectorizer(min_df=5, max_df=0.8)\n","X, y = Vectorize(vectorizer, text['text'], text['win_side'])\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","Classifier(X_train, X_test, y_train, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1czisymoKvOj","outputId":"df92ed8b-a6d4-4ff2-a038-763ea6aa0079"},"outputs":[{"name":"stdout","output_type":"stream","text":["--OVERSAMPLING--\n","            classifier  accuracy        f1  precision    recall\n","0  Logistic Regression  0.580952  0.661538   0.641791  0.682540\n","1          Naive Bayes  0.542857  0.606557   0.627119  0.587302\n","2           Linear SVC  0.533333  0.637037   0.597222  0.682540\n","3        Random Forest  0.580952  0.721519   0.600000  0.904762\n","4           Perceptron  0.580952  0.666667   0.637681  0.698413\n","--UNDERSAMPLING--\n","            classifier  accuracy        f1  precision    recall\n","0  Logistic Regression  0.609524  0.649573   0.703704  0.603175\n","1          Naive Bayes  0.561905  0.616667   0.649123  0.587302\n","2           Linear SVC  0.619048  0.636364   0.744681  0.555556\n","3        Random Forest  0.514286  0.495050   0.657895  0.396825\n","4           Perceptron  0.600000  0.625000   0.714286  0.555556\n"]}],"source":["# USING IMBLEARN\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","\n","vectorizer = TfidfVectorizer(min_df=5, max_df=0.8)\n","X, y = Vectorize(vectorizer, text['text'], text['win_side'])\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","\n","# Resample the training data\n","print('--OVERSAMPLING--')\n","ros = RandomOverSampler(random_state=0)\n","X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n","classifier = Classifier(X_train_resampled, X_test, y_train_resampled, y_test)\n","print(classifier)\n","\n","print('--UNDERSAMPLING--')\n","ros = RandomUnderSampler(random_state=0)\n","X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n","classifier = Classifier(X_train_resampled, X_test, y_train_resampled, y_test)\n","print(classifier)\n"]},{"cell_type":"markdown","metadata":{"id":"Gfs2aLjrKvOk"},"source":["Using Over/Undersampling not help much :-(\n","\n","NEXT!\n","Try using pretrained model from transformers.\n","\n","https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXEFsWFDKvOk","outputId":"1e351c53-c948-4f08-e08a-2bcab0a61a42","colab":{"referenced_widgets":["732e8b156e844c48894712df6db3123e","282064e0e0f441babdc33eb23c8bf53f","18739d362006418184273ef1427c9d88","11bd67d4ea134e28a6d25b0ef36b43fd","1a109f479da84faea5b6a0001d533659","d1f1eaecf9574199960183d3b2e415b9","98f23062a1ff47a38dff2f62ded3689c","fb1a042035f04f3596562267ebea761b","0f99a7ba67344fd6a9d37cb019918baf","d694b50b60d14b2792559f37f2326962","bd78d68d76dc4cb89553f1fccbe2e793","f95c0b8d4e394d669c4bed565a9e4558"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"732e8b156e844c48894712df6db3123e","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"282064e0e0f441babdc33eb23c8bf53f","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18739d362006418184273ef1427c9d88","version_major":2,"version_minor":0},"text/plain":["Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11bd67d4ea134e28a6d25b0ef36b43fd","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","c:\\Users\\rezar\\anaconda3\\envs\\torch_cuda\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a109f479da84faea5b6a0001d533659","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/624 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6994, 'learning_rate': 1e-05, 'epoch': 0.48}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1f1eaecf9574199960183d3b2e415b9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6757726669311523, 'eval_runtime': 2.6986, 'eval_samples_per_second': 38.909, 'eval_steps_per_second': 10.005, 'epoch': 0.48}\n","{'loss': 0.6696, 'learning_rate': 2e-05, 'epoch': 0.96}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98f23062a1ff47a38dff2f62ded3689c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6764814853668213, 'eval_runtime': 2.7597, 'eval_samples_per_second': 38.048, 'eval_steps_per_second': 9.784, 'epoch': 0.96}\n","{'loss': 0.7036, 'learning_rate': 3e-05, 'epoch': 1.44}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb1a042035f04f3596562267ebea761b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6685644388198853, 'eval_runtime': 2.7916, 'eval_samples_per_second': 37.613, 'eval_steps_per_second': 9.672, 'epoch': 1.44}\n","{'loss': 0.7099, 'learning_rate': 4e-05, 'epoch': 1.92}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f99a7ba67344fd6a9d37cb019918baf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7087978720664978, 'eval_runtime': 2.7839, 'eval_samples_per_second': 37.716, 'eval_steps_per_second': 9.698, 'epoch': 1.92}\n","{'loss': 0.6922, 'learning_rate': 5e-05, 'epoch': 2.4}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d694b50b60d14b2792559f37f2326962","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7302305698394775, 'eval_runtime': 2.7669, 'eval_samples_per_second': 37.949, 'eval_steps_per_second': 9.758, 'epoch': 2.4}\n","{'loss': 0.6753, 'learning_rate': 9.67741935483871e-06, 'epoch': 2.88}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd78d68d76dc4cb89553f1fccbe2e793","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.6546196341514587, 'eval_runtime': 2.7631, 'eval_samples_per_second': 38.001, 'eval_steps_per_second': 9.772, 'epoch': 2.88}\n","{'train_runtime': 136.1508, 'train_samples_per_second': 9.166, 'train_steps_per_second': 4.583, 'train_loss': 0.6893020134705764, 'epoch': 3.0}\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f95c0b8d4e394d669c4bed565a9e4558","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/27 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.6381\n","Precision: 0.6381\n","Recall: 1.0000\n","F1-Score: 0.7791\n"]}],"source":["import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n","\n","\n","# Load your data\n","data = pd.read_csv('text_clean.csv')\n","\n","# Split your data into training and testing sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(data['text'], data['win_side'], test_size=0.2, stratify=data['win_side'])\n","\n","# Initialize the BERT tokenizer\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n","\n","\n","# Tokenize the text data\n","train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)\n","\n","# Create PyTorch Class from dataset\n","class SCOTUSDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx]).long()\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = SCOTUSDataset(train_encodings, train_labels.tolist())\n","test_dataset = SCOTUSDataset(test_encodings, test_labels.tolist())\n","\n","# Initialize the BERT model for sequence classification\n","# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","# https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n","\n","# Set up training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=2, #changed to 2 because the GPU\n","    per_device_eval_batch_size=4,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=100,\n","    evaluation_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    save_steps=1000,\n","    load_best_model_at_end=True,\n",")\n","\n","# Create the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Evaluate the model\n","predictions = trainer.predict(test_dataset)\n","predicted_labels = predictions.label_ids\n","y_pred = (predictions.predictions.argmax(-1)).tolist()\n","\n","# Calculate performance metrics\n","accuracy = accuracy_score(test_labels, y_pred)\n","precision = precision_score(test_labels, y_pred)\n","recall = recall_score(test_labels, y_pred)\n","f1 = f1_score(test_labels, y_pred)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"4IYTPOc7KvOk"},"source":["Still below the baseline. Should redo the data preparation/or using other method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALfXXRNAKvOk"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}